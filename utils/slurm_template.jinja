#!/bin/bash
#SBATCH --job-name={{slurm_job_name}}
#SBATCH --get-user-env
#SBATCH --export=NONE
#SBATCH --clusters=cm4
#SBATCH --partition={{partition}}
#SBATCH --qos={{qos}}
#SBATCH --nodes={{num_nodes}}
#SBATCH --ntasks={{num_tasks}}
#SBATCH --cpus-per-task=38
#SBATCH --time=18:00:00
#SBATCH --output=logOutput_%j.log
#SBATCH --mail-type=end
#SBATCH --mail-user={{mail}}

echo "#==================================================#"
echo " num nodes: " $SLURM_JOB_NUM_NODES
echo " num tasks: " $SLURM_NTASKS
echo " cpus per task: " $SLURM_CPUS_PER_TASK
echo " nodes used: " $SLURM_JOB_NODELIST
echo " job cpus used: " $SLURM_JOB_CPUS_PER_NODE
echo "#==================================================#"
{% if use_mpi %}

module load intel-oneapi-mpi
unset I_MPI_PMI_LIBRARY
export I_MPI_JOB_RESPECT_PROCESS_PLACEMENT=0
{% endif %}

export OMP_NUM_THREADS=$SLURM_CPUS_PER_TASK
export OMP_PLACES=cores
export OMP_PROC_BIND=true

MD_FLEX_BINARY={{binary}}

mkdir -p output
cd output

{% for job in jobs %}

mkdir -p {{job.get_job_name()}} && cd {{job.get_job_name()}}
{{job.generate_command(use_mpi)}}
wait
cd ..

{% endfor %}

